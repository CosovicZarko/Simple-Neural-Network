# Simple-Neural-Network
Working with MNIST dataset to train a model which predicts unseen handwriteen digits. Despite being a short program, it delivers high accuracy, and this is mostly due the fact that the function used performs well on this dataset. Playing around with parameters brings interesting results. Increasing the training size increases the prediction accuracy, but there is a limit. Overtraining a model is possible, by lowering the variance of our estimates on the training set, we are increasing bias, and at some point this results in decreasing prediction accuracy on the test set. Another interesting thing to be seen by running the code, is that a training error may increase after a step, which may happen when the model is getting close to the solution.
