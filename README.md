# Simple-Neural-Network
Working with MNIST dataset to train model which predicts unseen handwriteen digits. Despite being a short program, it delivers high accuracy. That is mostly due the fact that the function used performs well on this dataset. Playing around with parameters brings interesting results. Increasing the training size increases the prediction accuracy, but there is a limit. Overtraining a model is possible, by lowering the variance of our estimates on the training set, we are increasing bias, and at some point this results in decreasing prediction accuracy on the test set. Another interesting thing to be seen by running the code, is that sometimes a training error is bigger than the previous training error. This usually happens when model is getting close to the solution.
